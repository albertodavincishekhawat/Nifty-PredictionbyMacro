{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\pytorch_forecasting\\models\\base_model.py:27: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, Baseline\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import lightning.pytorch as pl\n",
    "from torch.utils.data import DataLoader\n",
    "import torch_optimizer as optim\n",
    "from torch_optimizer import Ranger\n",
    "\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>vixPrice</th>\n",
       "      <th>vixChange%</th>\n",
       "      <th>niftyPrice</th>\n",
       "      <th>niftyChange %</th>\n",
       "      <th>n5day</th>\n",
       "      <th>n10day</th>\n",
       "      <th>n20day</th>\n",
       "      <th>n1day</th>\n",
       "      <th>n60day</th>\n",
       "      <th>...</th>\n",
       "      <th>RbiinterestAnticepation</th>\n",
       "      <th>USInflation Rate (%)</th>\n",
       "      <th>USInflationRate%chng</th>\n",
       "      <th>USInflation Rate (%)Anticepation</th>\n",
       "      <th>IndiaInflationRate(%)</th>\n",
       "      <th>IndiaInflationRate(%)chng</th>\n",
       "      <th>IndiaInflationRate(%)Anticepation</th>\n",
       "      <th>IndiaBudgetDatesAnticipation</th>\n",
       "      <th>IndiaElectionDatesAnticipation</th>\n",
       "      <th>UsElectionDatesAnticipation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>05-06-2008</td>\n",
       "      <td>30.32</td>\n",
       "      <td>1.68</td>\n",
       "      <td>4676.95</td>\n",
       "      <td>1.99</td>\n",
       "      <td>4835.3</td>\n",
       "      <td>5025.45</td>\n",
       "      <td>5135.50</td>\n",
       "      <td>4647.00</td>\n",
       "      <td>4864.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.651163</td>\n",
       "      <td>0</td>\n",
       "      <td>11.8</td>\n",
       "      <td>26.88172</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>06-06-2008</td>\n",
       "      <td>30.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4627.80</td>\n",
       "      <td>-1.05</td>\n",
       "      <td>4870.1</td>\n",
       "      <td>4946.55</td>\n",
       "      <td>5081.70</td>\n",
       "      <td>4761.20</td>\n",
       "      <td>4921.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.651163</td>\n",
       "      <td>0</td>\n",
       "      <td>11.8</td>\n",
       "      <td>26.88172</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>09-06-2008</td>\n",
       "      <td>32.43</td>\n",
       "      <td>6.96</td>\n",
       "      <td>4500.95</td>\n",
       "      <td>-2.74</td>\n",
       "      <td>4739.6</td>\n",
       "      <td>4875.05</td>\n",
       "      <td>4982.60</td>\n",
       "      <td>4709.65</td>\n",
       "      <td>4771.60</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.651163</td>\n",
       "      <td>0</td>\n",
       "      <td>11.8</td>\n",
       "      <td>26.88172</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10-06-2008</td>\n",
       "      <td>30.23</td>\n",
       "      <td>-6.78</td>\n",
       "      <td>4449.80</td>\n",
       "      <td>-1.14</td>\n",
       "      <td>4715.9</td>\n",
       "      <td>4859.80</td>\n",
       "      <td>5012.65</td>\n",
       "      <td>4747.05</td>\n",
       "      <td>4800.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.651163</td>\n",
       "      <td>0</td>\n",
       "      <td>11.8</td>\n",
       "      <td>26.88172</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11-06-2008</td>\n",
       "      <td>29.64</td>\n",
       "      <td>-1.95</td>\n",
       "      <td>4523.60</td>\n",
       "      <td>1.66</td>\n",
       "      <td>4585.6</td>\n",
       "      <td>4918.35</td>\n",
       "      <td>4957.80</td>\n",
       "      <td>4733.00</td>\n",
       "      <td>4865.90</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.651163</td>\n",
       "      <td>0</td>\n",
       "      <td>11.8</td>\n",
       "      <td>26.88172</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  vixPrice  vixChange%  niftyPrice  niftyChange %   n5day  \\\n",
       "0  05-06-2008     30.32        1.68     4676.95           1.99  4835.3   \n",
       "1  06-06-2008     30.32        0.00     4627.80          -1.05  4870.1   \n",
       "2  09-06-2008     32.43        6.96     4500.95          -2.74  4739.6   \n",
       "3  10-06-2008     30.23       -6.78     4449.80          -1.14  4715.9   \n",
       "4  11-06-2008     29.64       -1.95     4523.60           1.66  4585.6   \n",
       "\n",
       "    n10day   n20day    n1day   n60day  ...  RbiinterestAnticepation  \\\n",
       "0  5025.45  5135.50  4647.00  4864.25  ...                        0   \n",
       "1  4946.55  5081.70  4761.20  4921.40  ...                        0   \n",
       "2  4875.05  4982.60  4709.65  4771.60  ...                        0   \n",
       "3  4859.80  5012.65  4747.05  4800.40  ...                        0   \n",
       "4  4918.35  4957.80  4733.00  4865.90  ...                        0   \n",
       "\n",
       "   USInflation Rate (%)  USInflationRate%chng  \\\n",
       "0                   9.0              4.651163   \n",
       "1                   9.0              4.651163   \n",
       "2                   9.0              4.651163   \n",
       "3                   9.0              4.651163   \n",
       "4                   9.0              4.651163   \n",
       "\n",
       "   USInflation Rate (%)Anticepation  IndiaInflationRate(%)  \\\n",
       "0                                 0                   11.8   \n",
       "1                                 0                   11.8   \n",
       "2                                 0                   11.8   \n",
       "3                                 0                   11.8   \n",
       "4                                 0                   11.8   \n",
       "\n",
       "   IndiaInflationRate(%)chng  IndiaInflationRate(%)Anticepation  \\\n",
       "0                   26.88172                                  0   \n",
       "1                   26.88172                                  0   \n",
       "2                   26.88172                                  0   \n",
       "3                   26.88172                                  0   \n",
       "4                   26.88172                                  0   \n",
       "\n",
       "   IndiaBudgetDatesAnticipation  IndiaElectionDatesAnticipation  \\\n",
       "0                             0                               0   \n",
       "1                             1                               1   \n",
       "2                             2                               2   \n",
       "3                             3                               3   \n",
       "4                             4                               4   \n",
       "\n",
       "   UsElectionDatesAnticipation  \n",
       "0                            0  \n",
       "1                            1  \n",
       "2                            2  \n",
       "3                            3  \n",
       "4                            4  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"finaldata.csv\")\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step 2: Preprocess data for TimeSeriesDataSet\n",
    "Assuming columns: time_idx, target, group_id, and additional_covariates\n",
    "\n",
    "Modify these columns to match your dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing Date column to right data format \n",
    "data['Date'] = pd.to_datetime(data['Date'], format='%d-%m-%Y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['time_idx'] = np.arange(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Droping columns with string values which are already encoded into columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= data.drop('DayOfWeek', axis = 1)\n",
    "data= data.drop('month', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define parameters for the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_length = 240*5  # Number of past time steps to use for predictions\n",
    "max_prediction_length = 60  # Number of future steps to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confirmimg there are no nan values in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find indices of non-finite values\n",
    "non_finite_indices = np.where(~np.isfinite(data))\n",
    "result = [\n",
    "    {'Row': row, 'Column': data.columns[col], 'Value': data.iat[row, col]}\n",
    "    for row, col in zip(non_finite_indices[0], non_finite_indices[1])\n",
    "]\n",
    "\n",
    "# Display result\n",
    "for item in result:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 inf  values were found in  \n",
    "\n",
    "> {'Row': np.int64(2777), 'Column': 'T10Y2Y%chng', 'Value': np.float64(-inf)}\n",
    "\n",
    "> {'Row': np.int64(2782), 'Column': 'T10Y2Y%chng', 'Value': np.float64(inf)}\n",
    "\n",
    "> {'Row': np.int64(3485), 'Column': 'T10Y2Y%chng', 'Value': np.float64(-inf)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Corrected inf values mannually***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensure 'Date' column remains unchanged while transforming other columns are converted to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:, data.columns != 'Date'] = data.loc[:, data.columns != 'Date'].apply(\n",
    "    lambda x: (x * 100).astype(int) if x.dtype != 'int' else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a dummy group_id to the dataset\n",
    "data[\"dummy_group\"] = 0\n",
    "\n",
    "# Create TimeSeriesDataSet\n",
    "dataset = TimeSeriesDataSet(\n",
    "    data,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"niftyPrice\",\n",
    "    group_ids=[\"dummy_group\"],  # Use the dummy group column\n",
    "    time_varying_known_reals=[\n",
    "        \"vixPrice\", \"vixChange%\", \"niftyChange %\", \"n5day\", \"n10day\", \"n20day\", \"n1day\", \"n60day\",\n",
    "        \"nc5day\", \"nc10day\", \"nc20day\", \"nc1day\", \"nc60day\", \"v5day\", \"v10day\", \"v20day\", \"v1day\",\n",
    "        \"v60day\", \"vc5day\", \"vc10day\", \"vc20day\", \"vc1day\", \"vc60day\",\n",
    "        \"Tuesday\", \"Wednesday\", \"Friday\", \"Monday\", \"Thursday\", \"Saturday\", \"Sunday\", \"March\",\n",
    "        \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\",\n",
    "        \"January\", \"February\", \"n1day%\", \"n5day%\", \"n10day%\", \"n20day%\", \"n60day%\", \"cluster\",\n",
    "        \"Month\", \"GoldPrice\", \"GoldChange %\", \"CrudePrice\", \"CrudeChange %\", \"inrPrice\", \"inrChange %\",\n",
    "        \"diPrice\", \"diChange %\", \"T10Y2Y\", \"T10Y2Y%chng\", \"SnP500Price\", \"SnP500Change %\",\n",
    "        \"Fedinterest\", \"Fed%change\", \"FedinterestAnticepation\", \"Rbiinterest\", \"Rbi%change\",\n",
    "        \"RbiinterestAnticepation\", \"USInflation Rate (%)\", \"USInflationRate%chng\",\n",
    "        \"USInflation Rate (%)Anticepation\", \"IndiaInflationRate(%)\", \"IndiaInflationRate(%)chng\",\n",
    "        \"IndiaInflationRate(%)Anticepation\", \"IndiaBudgetDatesAnticipation\",\n",
    "        \"IndiaElectionDatesAnticipation\", \"UsElectionDatesAnticipation\"\n",
    "    ],\n",
    "    time_varying_unknown_reals=[\"niftyPrice\"],  # Keep niftyPrice only here\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    allow_missing_timesteps=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create validation set \n",
    "(predict=True) which means to predict the last max_prediction_length points in timefor each series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = TimeSeriesDataSet.from_dataset(dataset, data, predict=True, stop_randomization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataloaders for model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "c:\\Python39\\lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "c:\\Python39\\lib\\site-packages\\pytorch_forecasting\\models\\temporal_fusion_transformer\\__init__.py:171: UserWarning: In pytorch-forecasting models, on versions 1.1.X, the default optimizer defaults to 'adam', if pytorch_optimizer is not installed, otherwise it defaults to 'ranger' from pytorch_optimizer. From version 1.2.0, the default optimizer will be 'adam' regardless of whether pytorch_optimizer is installed, in order to minimize the number of dependencies in default parameter settings. Users who wish to ensure their code continues using 'ranger' as optimizer should ensure that pytorch_optimizer is installed, and set the optimizer parameter explicitly to 'ranger'.\n",
      "  super().__init__(loss=loss, logging_metrics=logging_metrics, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in TFT: 120.1k\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128  # set this between 32 to 128\n",
    "train_dataloader = dataset.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)\n",
    "\n",
    "# Step 3: Define Temporal Fusion Transformer model\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    dataset,\n",
    "    learning_rate=0.01,\n",
    "    hidden_size=16,  # Hidden layer size\n",
    "    attention_head_size=4,  # Number of attention heads\n",
    "    dropout=0.1,  # Dropout rate\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,  # Log training progress every 10 batches\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "\n",
    "print(f\"Number of parameters in TFT: {tft.size()/1e3:.1f}k\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create-baseline-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_18364\\2174382858.py:2: UserWarning: In pytorch-forecasting models, on versions 1.1.X, the default optimizer defaults to 'adam', if pytorch_optimizer is not installed, otherwise it defaults to 'ranger' from pytorch_optimizer. From version 1.2.0, the default optimizer will be 'adam' regardless of whether pytorch_optimizer is installed, in order to minimize the number of dependencies in default parameter settings. Users who wish to ensure their code continues using 'ranger' as optimizer should ensure that pytorch_optimizer is installed, and set the optimizer parameter explicitly to 'ranger'.\n",
      "  baseline_predictions = Baseline().predict(val_dataloader, return_y=True)\n",
      "c:\\Python39\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "c:\\Python39\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(164440.9219)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate baseline mean absolute error, i.e. predict next value as the last available value from the history\n",
    "baseline_predictions = Baseline().predict(val_dataloader, return_y=True)\n",
    "MAE()(baseline_predictions.output, baseline_predictions.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find optimal learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 65.1k\n"
     ]
    }
   ],
   "source": [
    "# configure network and trainer\n",
    "pl.seed_everything(42)\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"cpu\",\n",
    "    # clipping gradients is a hyperparameter and important to prevent divergance\n",
    "    # of the gradient for recurrent neural networks\n",
    "    gradient_clip_val=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    dataset,\n",
    "    # not meaningful for finding the learning rate but otherwise very important\n",
    "    learning_rate=0.01,\n",
    "    hidden_size=8,  # most important hyperparameter apart from learning rate\n",
    "    # number of attention heads. Set to up to 4 for large datasets\n",
    "    attention_head_size=4,\n",
    "    dropout=0.1,  # between 0.1 and 0.3 are good values\n",
    "    hidden_continuous_size=8,  # set to <= hidden_size\n",
    "    loss=QuantileLoss(),\n",
    "    optimizer=\"adam\",\n",
    "    # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    # reduce_on_plateau_patience=1000,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size() / 1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.cuda.device_count())  # Should return the number of GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\lightning\\pytorch\\loops\\utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "c:\\Python39\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Python39\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (21) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Python39\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "Finding best initial lr: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [29:02<00:00, 18.94s/it]`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Finding best initial lr: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [29:02<00:00, 17.43s/it]\n",
      "Learning rate set to 0.013489628825916528\n",
      "Restoring states from the checkpoint path at c:\\Users\\Asus\\Desktop\\Nifty-PredictionbyMacro\\.lr_find_6b60d5ab-0fdb-4d24-9a5a-410e04bdf6d3.ckpt\n",
      "Restored all states from the checkpoint at c:\\Users\\Asus\\Desktop\\Nifty-PredictionbyMacro\\.lr_find_6b60d5ab-0fdb-4d24-9a5a-410e04bdf6d3.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suggested learning rate: 0.013489628825916528\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_18364\\2949586714.py:13: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  fig.show()\n"
     ]
    }
   ],
   "source": [
    "# find optimal learning rate\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "res = Tuner(trainer).lr_find(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    "    max_lr=10.0,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------- code working above this point ---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "# Early stopping callback\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=5, verbose=True, mode=\"min\")\n",
    "\n",
    "# Trainer configuration\n",
    "trainer = Trainer(\n",
    "    max_epochs=30,  # Adjust epochs based on dataset size\n",
    "    accelerator=\"cpu\",  # Specify CPU usage\n",
    "    devices=1,  # Number of CPU cores to use\n",
    "    gradient_clip_val=0.1,\n",
    "    callbacks=[early_stop_callback],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "class TFTLightningModule(LightningModule):\n",
    "    def __init__(self, tft_model):\n",
    "        super().__init__()\n",
    "        self.tft_model = tft_model\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # The batch should already contain the necessary keys\n",
    "        return self.tft_model(batch)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y_hat = self(batch)\n",
    "        loss = self.tft_model.loss(y_hat, batch[\"target\"])\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y_hat = self(batch)\n",
    "        loss = self.tft_model.loss(y_hat, batch[\"target\"])\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.tft_model.parameters(), lr=1e-3)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type                      | Params | Mode\n",
      "---------------------------------------------------------------\n",
      "0 | tft_model | TemporalFusionTransformer | 65.1 K | eval\n",
      "---------------------------------------------------------------\n",
      "65.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "65.1 K    Total params\n",
      "0.260     Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "1755      Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtft_lightning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 539\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:575\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    569\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    571\u001b[0m     ckpt_path,\n\u001b[0;32m    572\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    573\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    574\u001b[0m )\n\u001b[1;32m--> 575\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:982\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    977\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 982\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    987\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1024\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m   1023\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m-> 1024\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1025\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m   1026\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1053\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1050\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[1;32m-> 1053\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1055\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:179\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop_run(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:144\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:433\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[1;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[0;32m    427\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    428\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[0;32m    432\u001b[0m )\n\u001b[1;32m--> 433\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:323\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 323\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    326\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:412\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mvalidation_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[31], line 18\u001b[0m, in \u001b[0;36mTFTLightningModule.validation_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m---> 18\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtft_model\u001b[38;5;241m.\u001b[39mloss(y_hat, batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[31], line 9\u001b[0m, in \u001b[0;36mTFTLightningModule.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pytorch_forecasting\\models\\temporal_fusion_transformer\\__init__.py:430\u001b[0m, in \u001b[0;36mTemporalFusionTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    427\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;124;03m    input dimensions: n_samples x time x variables\u001b[39;00m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 430\u001b[0m     encoder_lengths \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoder_lengths\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    431\u001b[0m     decoder_lengths \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_lengths\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    432\u001b[0m     x_cat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_cat\u001b[39m\u001b[38;5;124m\"\u001b[39m], x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_cat\u001b[39m\u001b[38;5;124m\"\u001b[39m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# concatenate in time dimension\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    tft_lightning,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the model\n",
    "tft.save_model(\"tft_model.pth\")\n",
    "print(\"Model training complete and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure network and trainer\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=1000, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator=\"cpu\",\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=50,  # coment in for training, running valiation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.01,\n",
    "    hidden_size=70,\n",
    "    attention_head_size=4,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    optimizer=\"adam\",\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size() / 1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, Baseline\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import lightning.pytorch as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_optimizer import Ranger\n",
    "\n",
    "from pytorch_lightning import Trainer, LightningModule\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "\n",
    "# Set seed for reproducibility\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(\"finaldata.csv\")\n",
    "data['Date'] = pd.to_datetime(data['Date'], format='%d-%m-%Y')\n",
    "\n",
    "# Add time index\n",
    "data['time_idx'] = np.arange(len(data))\n",
    "\n",
    "# Drop unused columns\n",
    "data = data.drop(['DayOfWeek', 'month'], axis=1)\n",
    "\n",
    "# Validate data\n",
    "if data.isna().sum().sum() > 0:\n",
    "    raise ValueError(\"Dataset contains NaN values. Please clean the data.\")\n",
    "\n",
    "# Normalize non-Date columns\n",
    "data.loc[:, data.columns != 'Date'] = data.loc[:, data.columns != 'Date'].apply(\n",
    "    lambda x: (x * 100).astype(int) if x.dtype != 'int' else x\n",
    ")\n",
    "\n",
    "# Add a dummy group ID for TimeSeriesDataSet\n",
    "data[\"dummy_group\"] = 0\n",
    "\n",
    "# Define TimeSeriesDataSet parameters\n",
    "max_encoder_length = 240*5  # Change based on dataset requirements\n",
    "max_prediction_length = 20  # Change based on dataset requirements\n",
    "\n",
    "dataset = TimeSeriesDataSet(\n",
    "    data,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"niftyPrice\",\n",
    "    group_ids=[\"dummy_group\"], \n",
    "    time_varying_known_reals=[\n",
    "        \"vixPrice\", \"vixChange%\", \"niftyChange %\", \"n5day\", \"n10day\", \"n20day\", \"n1day\", \"n60day\",\n",
    "        \"nc5day\", \"nc10day\", \"nc20day\", \"nc1day\", \"nc60day\", \"v5day\", \"v10day\", \"v20day\", \"v1day\",\n",
    "        \"v60day\", \"vc5day\", \"vc10day\", \"vc20day\", \"vc1day\", \"vc60day\",\n",
    "        \"Tuesday\", \"Wednesday\", \"Friday\", \"Monday\", \"Thursday\", \"Saturday\", \"Sunday\", \"March\",\n",
    "        \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\",\n",
    "        \"January\", \"February\", \"n1day%\", \"n5day%\", \"n10day%\", \"n20day%\", \"n60day%\", \"cluster\",\n",
    "        \"Month\", \"GoldPrice\", \"GoldChange %\", \"CrudePrice\", \"CrudeChange %\", \"inrPrice\", \"inrChange %\",\n",
    "        \"diPrice\", \"diChange %\", \"T10Y2Y\", \"T10Y2Y%chng\", \"SnP500Price\", \"SnP500Change %\",\n",
    "        \"Fedinterest\", \"Fed%change\", \"FedinterestAnticepation\", \"Rbiinterest\", \"Rbi%change\",\n",
    "        \"RbiinterestAnticepation\", \"USInflation Rate (%)\", \"USInflationRate%chng\",\n",
    "        \"USInflation Rate (%)Anticepation\", \"IndiaInflationRate(%)\", \"IndiaInflationRate(%)chng\",\n",
    "        \"IndiaInflationRate(%)Anticepation\", \"IndiaBudgetDatesAnticipation\",\n",
    "        \"IndiaElectionDatesAnticipation\", \"UsElectionDatesAnticipation\"\n",
    "    ],\n",
    "    time_varying_unknown_reals=[\"niftyPrice\"], \n",
    "    max_encoder_length=max_encoder_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    allow_missing_timesteps=False\n",
    ")\n",
    "\n",
    "# Validation dataset\n",
    "validation = TimeSeriesDataSet.from_dataset(dataset, data, predict=True, stop_randomization=True)\n",
    "\n",
    "# Create dataloaders\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"encoder_lengths\": torch.tensor([b[\"encoder_lengths\"] for b in batch]),\n",
    "        \"decoder_lengths\": torch.tensor([b[\"decoder_lengths\"] for b in batch]),\n",
    "        \"encoder_cat\": torch.stack([b[\"encoder_cat\"] for b in batch]),\n",
    "        \"decoder_cat\": torch.stack([b[\"decoder_cat\"] for b in batch]),\n",
    "        \"encoder_cont\": torch.stack([b[\"encoder_cont\"] for b in batch]),\n",
    "        \"decoder_cont\": torch.stack([b[\"decoder_cont\"] for b in batch]),\n",
    "        \"target\": torch.stack([b[\"target\"] for b in batch]),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "# train_dataloader = dataset.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in TFT: 120.1k                                \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the TemporalFusionTransformer\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    dataset,\n",
    "    learning_rate=0.01,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=4,\n",
    "    dropout=0.1,\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "\n",
    "print(f\"Number of parameters in TFT: {tft.size()/1e3:.1f}k\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define LightningModule wrapper\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "class TFTLightningModule(LightningModule):\n",
    "    def __init__(self, tft_model):\n",
    "        super().__init__()\n",
    "        self.tft_model = tft_model\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.tft_model(batch)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y_hat = self(batch)\n",
    "        loss = self.tft_model.loss(y_hat, batch[\"target\"])\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y_hat = self(batch)\n",
    "        loss = self.tft_model.loss(y_hat, batch[\"target\"])\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Ranger(self.tft_model.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize model wrapper\n",
    "tft_lightning = TFTLightningModule(tft)\n",
    "\n",
    "# Configure trainer\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\", verbose=True)\n",
    "trainer = Trainer(\n",
    "    max_epochs=30,\n",
    "    accelerator=\"cpu\",\n",
    "    devices=1,\n",
    "    gradient_clip_val=0.1,\n",
    "    callbacks=[early_stop_callback],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type                      | Params | Mode \n",
      "----------------------------------------------------------------\n",
      "0 | tft_model | TemporalFusionTransformer | 120 K  | train\n",
      "----------------------------------------------------------------\n",
      "120 K     Trainable params\n",
      "0         Non-trainable params\n",
      "120 K     Total params\n",
      "0.480     Total estimated model params size (MB)\n",
      "2367      Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtft_lightning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 539\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:575\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    569\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    571\u001b[0m     ckpt_path,\n\u001b[0;32m    572\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    573\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    574\u001b[0m )\n\u001b[1;32m--> 575\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:982\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    977\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 982\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    987\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1024\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m   1023\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m-> 1024\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1025\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m   1026\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1053\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1050\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[1;32m-> 1053\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1055\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:179\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop_run(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:144\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:433\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[1;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[0;32m    427\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    428\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[0;32m    432\u001b[0m )\n\u001b[1;32m--> 433\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:323\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 323\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    326\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:412\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mvalidation_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[43], line 19\u001b[0m, in \u001b[0;36mTFTLightningModule.validation_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m---> 19\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtft_model\u001b[38;5;241m.\u001b[39mloss(y_hat, batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[43], line 10\u001b[0m, in \u001b[0;36mTFTLightningModule.forward\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pytorch_forecasting\\models\\temporal_fusion_transformer\\__init__.py:430\u001b[0m, in \u001b[0;36mTemporalFusionTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    427\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;124;03m    input dimensions: n_samples x time x variables\u001b[39;00m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 430\u001b[0m     encoder_lengths \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoder_lengths\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    431\u001b[0m     decoder_lengths \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_lengths\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    432\u001b[0m     x_cat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_cat\u001b[39m\u001b[38;5;124m\"\u001b[39m], x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_cat\u001b[39m\u001b[38;5;124m\"\u001b[39m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# concatenate in time dimension\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "trainer.fit(\n",
    "    tft_lightning,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
